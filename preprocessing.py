import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import pandas as pd
import glob
import os
import re
from matplotlib.font_manager import FontProperties
import json
from sklearn.preprocessing import MinMaxScaler
import time
#è®¾ç½®å…¨å±€å­—ä½“
plt.rcParams["font.family"] = "Arial"  #ä»¿å®‹
plt.rcParams["axes.unicode_minus"] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf")  # SimHei å­—ä½“è·¯å¾„

sns.set(style="whitegrid")
def handle_missing_values(df_clean, file_id):
    missing = df_clean.isnull().sum()
    missing_ratio = missing / len(df_clean) * 100
    missing_info = pd.DataFrame({
        'å­—æ®µ': missing.index,
        'ç¼ºå¤±å€¼æ•°é‡': missing.values,
        'ç¼ºå¤±å€¼æ¯”ä¾‹ (%)': missing_ratio.values,
        'æ–‡ä»¶': file_id
    })
    # æ‰“å°ç¼ºå¤±å€¼ä¿¡æ¯è¡¨æ ¼
    print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    print(missing_info)
    df_clean = df_clean.dropna()
    print(f"å¤„ç†ç©ºç¼ºå€¼åå‰©ä½™è®°å½•æ•°: {len(df_clean)}\n")
    return df_clean, missing_info

# #ç¼ºå¤±å€¼å¤„ç†
# def handle_missing_values(df_clean):
#     missing = df_clean.isnull().sum()
#     # è®¡ç®—ç¼ºå¤±å€¼æ¯”ä¾‹
#     missing_ratio = missing / len(df_clean) * 100
    
#     # åˆ›å»ºä¸€ä¸ª DataFrame æ¥å­˜å‚¨ç¼ºå¤±å€¼ä¿¡æ¯
#     missing_info = pd.DataFrame({
#         'ç¼ºå¤±å€¼æ•°é‡': missing,
#         'ç¼ºå¤±å€¼æ¯”ä¾‹ (%)': missing_ratio
#     })
    
#     # æ‰“å°ç¼ºå¤±å€¼ä¿¡æ¯è¡¨æ ¼
#     print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
#     print(missing_info)
    
#     # å¤„ç†ç©ºç¼ºå€¼
#     df_clean = df_clean.dropna()
#     print(f"å¤„ç†ç©ºç¼ºå€¼åå‰©ä½™è®°å½•æ•°: {len(df_clean)}\n")
#     return df_clean
def normalize_numeric_columns(df, columns):
    scaler = MinMaxScaler()
    df[columns] = scaler.fit_transform(df[columns])
    return df

def extract_purchase_features(df):
    """ä» purchase_history ä¸­æå– average_price å’Œ item_count"""
    avg_prices = []
    item_counts = []

    for record in df['purchase_history']:
        try:
            data = json.loads(record)
            avg_prices.append(data.get('average_price', np.nan))
            item_counts.append(len(data.get('items', [])))
        except Exception:
            avg_prices.append(np.nan)
            item_counts.append(np.nan)

    df['purchase_avg_price'] = avg_prices
    df['purchase_item_count'] = item_counts
    return df
def is_valid_phone(phone):
    return bool(re.match(r"\d{3}-\d{3}-\d{4}$", phone))
def is_valid_email(email):
    return bool(re.match(r"[^@]+@[^@]+\.[^@]+", email))


def remove_outliers_iqr(df, column):
    """ä½¿ç”¨ IQR æ–¹æ³•å»é™¤æŸåˆ—çš„å¼‚å¸¸å€¼"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = max(Q1 - 1.5 * IQR, 0)
    upper_bound = Q3 + 1.5 * IQR
    print(f"{column} è¿‡æ»¤èŒƒå›´: {lower_bound:.2f} ~ {upper_bound:.2f}")
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

def draw_boxplot(df, col, file_id, info):
    plt.figure(figsize=(6, 4))
    sns.boxplot(y=df[col])
    plt.title(f"{col} ç®±çº¿å›¾ - {file_id}", fontproperties=font)
    plt.tight_layout()

    # æ„å»ºä¿å­˜è·¯å¾„
    save_path = f"./result_images/{info}boxplot_{col}_{file_id}.png"
    save_dir = os.path.dirname(save_path)
    
    # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºç›®å½•
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    plt.savefig(save_path)  # ä¿å­˜ä¸ºå›¾ç‰‡
    plt.close()


# æ–‡ä»¶å¤¹è·¯å¾„
info = '30G_data'
folder_path = 'data/' + info

# æ–°ç›®å½•è·¯å¾„
cleaned_folder = 'data/cleaned/' + info
os.makedirs(cleaned_folder, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰

# è·å–æ‰€æœ‰ parquet æ–‡ä»¶è·¯å¾„
parquet_files = sorted(glob.glob(os.path.join(folder_path, 'part-*.parquet')))

all_high_value_users = []
missing_summary_list = []
outlier_summary_list = []

#è®°å½•å¼€å§‹æ—¶é—´å¹¶æ‰“å°
start_time = time.time()
print("å¼€å§‹æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
for file in parquet_files:
    print("*****************************************************************")
    print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file}")
    file_id = os.path.basename(file).split('.')[0]  # å¦‚ part-00000
    filename = os.path.basename(file)
    try:
        # è¯»å– parquet æ–‡ä»¶
        df_clean = pd.read_parquet(file, engine='pyarrow')
        print(df_clean.info())

        df_clean, missing_info = handle_missing_values(df_clean, file_id)
        missing_summary_list.append(missing_info)

    # å¤„ç†æ•°å€¼å¼‚å¸¸å€¼å­—æ®µ
        for col in ['age', 'income', 'credit_score']:
            print(f"\nå¤„ç†å­—æ®µï¼š{col}")
            before = len(df_clean)
            df_clean = remove_outliers_iqr(df_clean, col)
            after = len(df_clean)
            removed = before - after
            print(f"åˆ é™¤å¼‚å¸¸å€¼åï¼š{before - after} è¡Œè¢«ç§»é™¤")
            ratio = round(removed / before * 100, 2)
            #è®¡ç®—å¼‚å¸¸å€¼æ¯”ä¾‹
            print(f"å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š{round((after - before) / before * 100, 2)}%")
            draw_boxplot(df_clean, col, file_id, info)

            outlier_summary_list.append({
            'æ–‡ä»¶': file_id,
            'å­—æ®µ': col,
            'å¼‚å¸¸å€¼æ•°é‡': removed,
            'å¼‚å¸¸å€¼æ¯”ä¾‹ (%)': ratio
            })
    # å¤„ç†éæ•°å€¼å­—æ®µ
      
        expected_cols = ['gender', 'email', 'phone_number']
        existing = set(df_clean.columns).intersection(expected_cols)

        if 'gender' in existing:
            t1 = len(df_clean)
            df_clean = df_clean[df_clean['gender'].isin(['ç”·', 'å¥³'])]
            t2 = len(df_clean)
            print(f"åˆ é™¤æ€§åˆ«å¼‚å¸¸å€¼åï¼š{t1 - t2} è¡Œè¢«ç§»é™¤")

            outlier_summary_list.append({
            'æ–‡ä»¶': file_id,
            'å­—æ®µ': 'gender',
            'å¼‚å¸¸å€¼æ•°é‡': t1 - t2,
            'å¼‚å¸¸å€¼æ¯”ä¾‹ (%)': (t1 - t2) / t1 * 100
            })

        if 'email' in existing:
            t1  = len(df_clean)
            df_clean = df_clean[df_clean['email'].apply(is_valid_email)]
            t2 = len(df_clean)
            print(f"åˆ é™¤é‚®ç®±å¼‚å¸¸å€¼åï¼š{t1 - t2} è¡Œè¢«ç§»é™¤")
            outlier_summary_list.append({
            'æ–‡ä»¶': file_id,
            'å­—æ®µ': 'email',
            'å¼‚å¸¸å€¼æ•°é‡': t1 - t2,
            'å¼‚å¸¸å€¼æ¯”ä¾‹ (%)': (t1 - t2) / t1 * 100
            })
        if 'phone_number' in existing:
            t1 = len(df_clean)
            df_clean = df_clean[df_clean['phone_number'].apply(is_valid_phone)]
            t2 = len(df_clean)
            print(f"åˆ é™¤ç”µè¯å·ç å¼‚å¸¸å€¼åï¼š{t1 - t2} è¡Œè¢«ç§»é™¤")
            outlier_summary_list.append({
            'æ–‡ä»¶': file_id,
            'å­—æ®µ': 'phone_number',
            'å¼‚å¸¸å€¼æ•°é‡': t1 - t2,
            'å¼‚å¸¸å€¼æ¯”ä¾‹ (%)': (t1 - t2) / t1 * 100
            })
         # æå–è´­ä¹°ä¿¡æ¯
        df_clean = extract_purchase_features(df_clean)

        #å¤„ç†ç™»è®°æ—¶é—´
        # ç¡®ä¿ registration_date æ˜¯ datetime ç±»å‹
        df_clean['registration_date'] = pd.to_datetime(df_clean['registration_date'])
        # è·å–å½“å‰æ—¥æœŸ
        now = pd.to_datetime("today")
        # è®¡ç®—æ³¨å†Œå¤©æ•°ï¼ˆæ³¨å†Œåˆ°ç°åœ¨ç»è¿‡äº†å¤šå°‘å¤©ï¼‰
        df_clean['registration_days'] = (now - df_clean['registration_date']).dt.days


        # é€‰æ‹©è¦å½’ä¸€åŒ–çš„å­—æ®µ
        numeric_columns = ['registration_days', 'age', 'income', 'credit_score', 'purchase_avg_price', 'purchase_item_count']
         # å½’ä¸€åŒ–æ•°å€¼å­—æ®µ
        df_clean = normalize_numeric_columns(df_clean, numeric_columns)
        print(f"âœ… æ•°æ®å·²å½’ä¸€åŒ–")

        df_clean['is_active_num'] = df_clean['is_active'].astype(int)

        #è®¡ç®—ç»¼åˆå¾—åˆ†
        df_clean['user_value_score'] = (
        0.25 * df_clean['income'] +
        0.20 * df_clean['credit_score'] +
        0.20 * df_clean['purchase_avg_price'] +
        0.15 * df_clean['purchase_item_count'] +
        0.10 * df_clean['is_active_num'] +
        0.10 * (1 - df_clean['registration_days'])  # æ³¨å†Œè¶Šæ—©åˆ†æ•°è¶Šé«˜
        )

        threshold = df_clean['user_value_score'].quantile(0.90)
        high_value_users = df_clean[df_clean['user_value_score'] >= threshold]
        print(f"ğŸ¯ é«˜ä»·å€¼ç”¨æˆ·æ•°é‡ï¼š{len(high_value_users)}")

        # åŠ å…¥åˆ—è¡¨
        all_high_value_users.append(high_value_users)
    except Exception as e:
        print(f"è¯»å–å¤±è´¥: {file}ï¼Œé”™è¯¯ï¼š{e}")


# æ‹¼æ¥æ‰€æœ‰é«˜ä»·å€¼ç”¨æˆ·
final_high_value_users = pd.concat(all_high_value_users, ignore_index=True)

# ä¿å­˜åˆ°æ–‡ä»¶
output_path = 'data/' + info + 'high_value_users.parquet'
final_high_value_users.to_parquet(output_path, engine='pyarrow', index=False)
print(f"âœ… æ‰€æœ‰é«˜ä»·å€¼ç”¨æˆ·æ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")

# ç¼ºå¤±å€¼æ€»è¡¨
missing_summary_df = pd.concat(missing_summary_list, ignore_index=True)
missing_summary_df = missing_summary_df[missing_summary_df['ç¼ºå¤±å€¼æ•°é‡'] > 0]
missing_summary_df.to_csv('data/' + info + 'result_missing_summary.csv', index=False, encoding='utf-8-sig')
print("ğŸ“Š ç¼ºå¤±å€¼ç»Ÿè®¡è¡¨å·²ä¿å­˜ä¸º result_missing_summary.csv")

# å¼‚å¸¸å€¼æ€»è¡¨
outlier_summary_df = pd.DataFrame(outlier_summary_list)
outlier_summary_df = outlier_summary_df[outlier_summary_df['å¼‚å¸¸å€¼æ•°é‡'] > 0]
outlier_summary_df.to_csv('data/' + info + 'result_outlier_summary.csv', index=False, encoding='utf-8-sig')
print("ğŸ“Š å¼‚å¸¸å€¼ç»Ÿè®¡è¡¨å·²ä¿å­˜ä¸º result_outlier_summary.csv")

#è®°å½•ç»“æŸæ—¶é—´å¹¶æ‰“å°
end_time = time.time()
print("ç»“æŸæ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
#æ‰“å°æ€»æ—¶é•¿
total_time = end_time - start_time
#æ‰“å°æ€»æ—¶é•¿ï¼Œä»¥åˆ†é’Ÿä¸ºå•ä½
print("æ€»è€—æ—¶:", total_time / 60, "åˆ†é’Ÿ")
